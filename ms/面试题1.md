# 浏览器网址打开流程详解

浏览器打开网址的流程是一个涉及多个步骤的复杂过程，主要包括以下关键环节：

---

### **1. 用户输入URL**
- **输入处理**：用户在地址栏输入网址（如 `https://www.example.com`），浏览器会先检查输入的内容是否为有效的URL。如果不是，可能触发搜索引擎搜索。
- **协议补全**：浏览器可能自动补全协议（如 `http://` 或 `https://`）。

---

### **2. 解析URL**
浏览器解析URL的各个部分：
- **协议**：决定使用HTTP/HTTPS或其他协议（如FTP）。
- **域名**：如 `www.example.com`。
- **端口**：默认HTTP为80，HTTPS为443。
- **路径和查询参数**：如 `/page?id=1`。

---

### **3. DNS域名解析**
将域名转换为IP地址：
1. **浏览器缓存**：检查本地是否缓存过该域名的IP。
2. **系统缓存**：查询操作系统（如Windows的`hosts`文件或Linux的`/etc/hosts`）。
3. **路由器缓存**：检查本地路由器的DNS缓存。
4. **ISP的DNS服务器**：向互联网服务提供商（ISP）的DNS服务器发起递归查询。
5. **根域名服务器→顶级域（TLD）→权威DNS服务器**：若未缓存，则从根域名开始逐级查询，最终获取域名的IP地址。

---

### **4. 建立TCP连接**
通过**三次握手**与服务器建立TCP连接：
1. **SYN**：客户端发送同步报文到服务器。
2. **SYN-ACK**：服务器回应同步确认报文。
3. **ACK**：客户端发送确认报文，连接建立。
- **HTTPS额外步骤**：若为HTTPS，会进行TLS握手（协商加密算法、验证证书、交换密钥等）。

---

### **5. 发送HTTP请求**
浏览器向服务器发送HTTP请求报文，例如：
```http
GET /page HTTP/1.1
Host: www.example.com
User-Agent: Chrome/...
Accept: text/html, */*
```
- 可能包含请求头（Headers）、请求体（Body，如POST请求的数据）。

---

### **6. 服务器处理请求**
服务器（如Nginx、Apache）处理请求：
- **静态资源**：直接返回文件（如HTML、图片）。
- **动态资源**：通过后端程序（如PHP、Node.js）生成响应。
- **数据库查询**：如需动态数据，服务器可能查询数据库。

---

### **7. 服务器返回HTTP响应**
服务器返回响应报文，包含：
- **状态码**：如 `200 OK`（成功）、`404 Not Found`（未找到资源）。
- **响应头**：内容类型（`Content-Type`）、缓存控制（`Cache-Control`）等。
- **响应体**：HTML内容或其他数据。

---

### **8. 浏览器渲染页面**
1. **解析HTML**：构建DOM树（文档对象模型）。
2. **解析CSS**：生成CSSOM树（CSS对象模型）。
3. **合并DOM与CSSOM**：构建渲染树（Render Tree）。
4. **布局（Layout）**：计算元素的位置和大小。
5. **绘制（Painting）**：将像素绘制到屏幕上。
6. **执行JavaScript**：可能阻塞渲染（需通过`async`/`defer`优化）。

---

### **9. 关闭连接**
- **HTTP/1.1**：默认保持连接（Keep-Alive），可复用。
- **HTTP/2**：多路复用，减少连接开销。
- **四次挥手**：若关闭TCP连接，客户端和服务器交换FIN/ACK报文。

---

### **关键优化技术**
- **DNS预解析**：`<link rel="dns-prefetch" href="//example.com">`。
- **CDN加速**：就近获取资源。
- **缓存策略**：利用HTTP缓存头（如`Cache-Control`、`ETag`）。
- **HTTP/2**：多路复用、头部压缩。

---

### **流程图概览**
```
用户输入 → URL解析 → DNS查询 → TCP握手 → TLS握手（HTTPS） → 
HTTP请求 → 服务器处理 → HTTP响应 → 渲染 → 关闭连接
```

这个流程涉及网络协议、操作系统、浏览器引擎等多个层面的协作，实际细节可能因浏览器或网络环境而异。



# redis支持哪些数据类型

Redis 支持多种数据类型，每种类型针对不同的使用场景设计。以下是 Redis 的主要数据类型及其特点：

---

### **1. String（字符串）**
- **描述**：最基本的数据类型，可以存储文本、数字或二进制数据（最大 **512MB**）。
- **常用命令**：
  - `SET key value`：设置值。
  - `GET key`：获取值。
  - `INCR key`：将值递增（原子操作）。
  - `APPEND key value`：追加内容。
- **应用场景**：
  - 缓存（如用户会话、HTML片段）。
  - 计数器（如文章阅读量）。
  - 分布式锁（通过 `SETNX` 实现）。

---

### **2. Hash（哈希表）**
- **描述**：键值对集合，适合存储对象（如用户信息）。
- **常用命令**：
  - `HSET key field value`：设置字段值。
  - `HGET key field`：获取字段值。
  - `HGETALL key`：获取所有字段和值。
  - `HDEL key field`：删除字段。
- **应用场景**：
  - 存储对象（如用户属性：`name`, `age`, `email`）。
  - 避免频繁序列化/反序列化。

---

### **3. List（列表）**
- **描述**：有序的字符串列表，基于双向链表实现，支持两端插入/删除。
- **常用命令**：
  - `LPUSH key value`：从左侧插入。
  - `RPUSH key value`：从右侧插入。
  - `LPOP key`：从左侧弹出。
  - `LRANGE key start stop`：获取范围数据。
- **应用场景**：
  - 消息队列（结合 `LPUSH` + `BRPOP`）。
  - 最新消息排行（如微博时间线）。

---

### **4. Set（集合）**
- **描述**：无序且唯一的字符串集合，支持交并差运算。
- **常用命令**：
  - `SADD key member`：添加成员。
  - `SMEMBERS key`：获取所有成员。
  - `SINTER key1 key2`：求交集。
  - `SISMEMBER key member`：判断成员是否存在。
- **应用场景**：
  - 标签系统（如文章标签）。
  - 好友关系（共同好友计算）。

---

### **5. Sorted Set（有序集合）**
- **描述**：在 Set 基础上为每个成员关联一个分数（score），按分数排序。
- **常用命令**：
  - `ZADD key score member`：添加成员。
  - `ZRANGE key start stop`：按分数范围获取成员。
  - `ZRANK key member`：获取成员排名。
- **应用场景**：
  - 排行榜（如游戏分数排名）。
  - 延迟队列（用分数表示执行时间）。

---

### **6. Bitmaps（位图）**
- **描述**：通过二进制位操作存储布尔值，节省空间。
- **常用命令**：
  - `SETBIT key offset value`：设置某位的值（0/1）。
  - `GETBIT key offset`：获取某位的值。
  - `BITCOUNT key`：统计值为1的位数。
- **应用场景**：
  - 用户签到记录。
  - 实时活跃用户统计。

---

### **7. HyperLogLog（基数统计）**
- **描述**：用于估算集合中不重复元素的数量（误差率约0.81%）。
- **常用命令**：
  - `PFADD key element`：添加元素。
  - `PFCOUNT key`：估算基数。
  - `PFMERGE destkey sourcekey`：合并多个集合。
- **应用场景**：
  - 大规模UV统计（如网站独立访客数）。

---

### **8. Geospatial（地理空间）**
- **描述**：存储地理位置（经纬度），支持距离计算、范围查询。
- **常用命令**：
  - `GEOADD key longitude latitude member`：添加位置。
  - `GEODIST key member1 member2`：计算距离。
  - `GEORADIUS key longitude latitude radius`：查询附近位置。
- **应用场景**：
  - 附近的人、商家。
  - 地理位置索引。

---

### **9. Stream（流）**
- **描述**：Redis 5.0引入的日志流数据结构，支持消息持久化和消费者组。
- **常用命令**：
  - `XADD key * field value`：添加消息。
  - `XREAD COUNT n STREAMS key ID`：读取消息。
  - `XGROUP CREATE`：创建消费者组。
- **应用场景**：
  - 消息队列（类似Kafka）。
  - 事件溯源（Event Sourcing）。

---

### **总结**
| 数据类型    | 特点                     | 典型场景           |
| ----------- | ------------------------ | ------------------ |
| String      | 简单键值                 | 缓存、计数器       |
| Hash        | 结构化对象               | 用户属性存储       |
| List        | 有序、可重复             | 消息队列、时间线   |
| Set         | 无序、唯一               | 标签、好友关系     |
| Sorted Set  | 有序、唯一（按分数排序） | 排行榜、延迟任务   |
| Bitmaps     | 位操作                   | 签到、布尔统计     |
| HyperLogLog | 基数估算                 | UV统计             |
| Geospatial  | 地理位置                 | 附近的人、商家     |
| Stream      | 消息流、消费者组         | 事件日志、消息队列 |

Redis的多数据类型使其成为灵活的缓存和存储解决方案，选择合适的数据类型可以显著提升性能和资源利用率。

# Redis 的持久化机制

Redis 的持久化机制是为了保证数据在服务器重启后仍能恢复，避免数据丢失。Redis 提供了两种主要的持久化方式：**RDB（快照）** 和 **AOF（日志追加）**，以及混合持久化模式。以下是它们的详细说明：

---

## **1. RDB（Redis Database）**
### **原理**
- RDB 是 Redis 的**快照持久化**方式，会在指定时间间隔内将内存中的数据生成一个**二进制快照文件**（默认名为 `dump.rdb`）。
- 生成 RDB 文件时，Redis 会 fork 一个子进程来执行持久化，主进程继续处理请求，不影响性能。

### **触发方式**
- **手动触发**：
  - `SAVE`：阻塞 Redis 主进程，直到 RDB 文件创建完成（生产环境慎用）。
  - `BGSAVE`（Background Save）：后台异步生成 RDB 文件，不阻塞主进程（推荐）。
- **自动触发**：
  - 在 `redis.conf` 中配置 `save` 规则，例如：
    ```conf
    save 900 1     # 900秒（15分钟）内至少1个key被修改，则触发BGSAVE
    save 300 10    # 300秒（5分钟）内至少10个key被修改，则触发BGSAVE
    save 60 10000  # 60秒内至少10000个key被修改，则触发BGSAVE
    ```

### **优点**
- **性能高**：RDB 是二进制文件，恢复速度快。
- **适合备份**：文件紧凑，适合全量备份和灾难恢复。
- **减少磁盘 I/O**：相比 AOF，RDB 文件较小，写入频率低。

### **缺点**
- **数据可能丢失**：如果 Redis 宕机，最后一次快照之后的数据会丢失（取决于 `save` 配置）。
- **fork 可能阻塞**：如果数据量很大，fork 子进程时可能会短暂阻塞主进程（尤其在内存较大的情况下）。

---

## **2. AOF（Append Only File）**
### **原理**
- AOF 记录 Redis 的**所有写操作命令**（如 `SET`、`DEL`），以文本日志的形式追加到文件（默认 `appendonly.aof`）。
- Redis 重启时会**重放 AOF 文件中的命令**来恢复数据。

### **触发方式**
- **同步策略**（`appendfsync` 选项）：
  - `always`：每次写命令都同步到磁盘（最安全，但性能最差）。
  - `everysec`（默认）：每秒同步一次（平衡安全性和性能）。
  - `no`：由操作系统决定何时同步（最快，但可能丢失数据）。

### **AOF 重写（Rewrite）**
- AOF 文件会不断增长，Redis 提供 `BGREWRITEAOF` 命令来**压缩 AOF 文件**（移除冗余命令）。
- 自动触发规则（`redis.conf`）：
  ```conf
  auto-aof-rewrite-percentage 100  # AOF 文件增长超过100%时触发重写
  auto-aof-rewrite-min-size 64mb   # AOF 文件最小64MB才触发重写
  ```

### **优点**
- **数据更安全**：可以配置 `appendfsync always` 实现几乎零数据丢失。
- **可读性强**：AOF 是文本文件，便于人工查看和修复。

### **缺点**
- **文件更大**：AOF 通常比 RDB 文件大。
- **恢复较慢**：重放 AOF 日志比加载 RDB 慢。
- **写入压力大**：高并发写入时，AOF 可能影响性能。

---

## **3. 混合持久化（RDB + AOF）**
Redis 4.0+ 支持混合持久化，结合 RDB 和 AOF 的优点：
- **AOF 文件包含两部分**：
  1. **RDB 格式的全量数据**（快照）。
  2. **增量 AOF 日志**（记录快照后的写操作）。
- **触发方式**：
  - 在 `redis.conf` 中启用：
    ```conf
    aof-use-rdb-preamble yes
    ```
- **优点**：
  - 恢复速度快（先加载 RDB，再重放少量 AOF）。
  - 数据安全性高（AOF 记录增量数据）。

---

## **4. 如何选择持久化方式？**
| **持久化方式** | **适用场景**                         | **优缺点**             |
| -------------- | ------------------------------------ | ---------------------- |
| **RDB**        | 适合大规模数据备份、允许少量数据丢失 | 恢复快，但可能丢失数据 |
| **AOF**        | 需要高数据安全性（如金融场景）       | 数据更安全，但恢复慢   |
| **混合持久化** | Redis 4.0+，兼顾速度和安全性         | 推荐生产环境使用       |

### **推荐配置（生产环境）**
```conf
save 900 1           # RDB 触发规则（可选）
appendonly yes       # 开启 AOF
appendfsync everysec # 每秒同步
aof-use-rdb-preamble yes # 开启混合持久化
```

---

## **5. 数据恢复流程**
1. **如果只启用 RDB**：
   - Redis 重启时自动加载 `dump.rdb`。
2. **如果只启用 AOF**：
   - Redis 重启时重放 `appendonly.aof`。
3. **如果同时启用 RDB + AOF**：
   - Redis **优先使用 AOF** 恢复（因为 AOF 数据更完整）。

---

## **6. 注意事项**
- **监控持久化状态**：
  - `INFO persistence` 查看 `rdb_last_save_time` 和 `aof_last_write_status`。
- **备份策略**：
  - 定期备份 RDB/AOF 文件到远程存储（如 S3、OSS）。
- **避免 fork 阻塞**：
  - 如果 Redis 内存很大（如几十GB），`BGSAVE` 或 `BGREWRITEAOF` 可能导致短暂阻塞，建议在低峰期执行。

---

### **总结**
- **RDB**：适合备份，恢复快，但可能丢数据。
- **AOF**：数据更安全，但恢复慢，文件较大。
- **混合持久化**（推荐）：结合 RDB 和 AOF 的优点，适合生产环境。

根据业务需求选择合适的持久化策略，并做好监控和备份！



# Python中哪些数据类型不变

在 Python 中，**不可变数据类型（Immutable Types）** 是指一旦创建，其值就不能被修改的数据类型。如果尝试修改，Python 实际上会创建一个新的对象。以下是 Python 中的主要不可变数据类型：

---

### **1. 数值类型（Numeric Types）**
- **`int`（整数）**  
  
  ```python
  x = 10
  x += 1  # 实际上是创建了一个新的 int 对象，而不是修改原来的 x
  ```
- **`float`（浮点数）**  
  ```python
  y = 3.14
  y = y + 1.0  # 创建新的 float 对象
  ```
- **`bool`（布尔值）**  
  
  ```python
  flag = True
  flag = False  # 创建新的 bool 对象
  ```
- **`complex`（复数）**  
  ```python
  z = 1 + 2j
  z = z + (3 + 4j)  # 创建新的 complex 对象
  ```

---

### **2. 字符串（`str`）**
字符串一旦创建，就不能修改其中的字符：
```python
s = "hello"
s[0] = "H"  # 报错！TypeError: 'str' object does not support item assignment

# 只能通过拼接等方式生成新字符串
s = "H" + s[1:]  # 新对象 "Hello"
```

---

### **3. 元组（`tuple`）**
元组是不可变的列表，不能修改、添加或删除元素：
```python
t = (1, 2, 3)
t[0] = 10  # 报错！TypeError: 'tuple' object does not support item assignment

# 只能创建新元组
t = (10,) + t[1:]  # (10, 2, 3)
```

---

### **4. 冻结集合（`frozenset`）**
`frozenset` 是不可变的集合，不能添加或删除元素：

```python
fs = frozenset([1, 2, 3])
fs.add(4)  # 报错！AttributeError: 'frozenset' object has no attribute 'add'
```

---

### **不可变类型的共同特点**
1. **哈希支持**：不可变对象可以作为字典的键（`dict`）或集合的元素（`set`），因为它们的哈希值不变。
   
   ```python
   d = {(1, 2): "tuple as key"}  # 元组可作为键
   s = {1, "hello", (2, 3)}      # 集合可包含不可变元素
   ```
2. **内存优化**：Python 会对小整数（如 `-5` 到 `256`）和短字符串进行缓存，复用相同对象。

---

### **常见可变数据类型（对比）**
| 可变类型（Mutable） | 不可变类型（Immutable） |
| ------------------- | ----------------------- |
| `list`              | `tuple`                 |
| `dict`              | `frozenset`             |
| `set`               | `str`                   |
| `bytearray`         | `bytes`                 |

---

### **为什么需要不可变类型？**
1. **线程安全**：不可变对象在多线程环境下无需加锁。
2. **哈希支持**：允许作为字典的键。
3. **性能优化**：如字符串驻留（interning）、小整数缓存。

---

### **如何判断一个对象是否可变？**
使用 `id()` 观察对象的内存地址是否变化：
```python
x = (1, 2)
print(id(x))  # 输出内存地址
x += (3,)     # 创建新元组
print(id(x))  # 内存地址改变（新对象）
```

或直接尝试修改：
```python
try:
    x[0] = 10  # 如果报错，则是不可变类型
except TypeError:
    print("Immutable!")
```

---

### **总结**
Python 的不可变数据类型包括：
- **`int`、`float`、`bool`、`complex`**
- **`str`**
- **`tuple`**
- **`frozenset`**
- **`bytes`**

这些类型的特点是**创建后不能修改**，任何“修改”操作都会生成新对象。



# 单机实现高并发有哪些实现方法



在单机环境下实现高并发，需要通过优化代码、合理利用系统资源、选择高效的技术栈来最大化单台服务器的处理能力。以下是具体的实现方法和关键技术：

---

### **1. I/O 模型优化**
#### **① 非阻塞 I/O + 多路复用**
- **原理**：通过 `select`/`poll`/`epoll`（Linux）或 `kqueue`（BSD）监听多个 I/O 事件，避免线程阻塞。
- **适用场景**：高并发网络请求（如 Web 服务器）。
- **实现**：
  - **C/C++**：`libevent`、`libuv`。
  - **Java**：NIO（`Selector`）。
  - **Python**：`asyncio`（基于事件循环）。
  - **Go**：原生 Goroutine + `epoll`。

#### **② 异步 I/O（AIO）**
- **原理**：系统内核完成 I/O 操作后通知应用，完全无阻塞（如 Linux `io_uring`）。
- **示例**：
  ```python
  # Python 3.7+ 的异步文件读写
  async with aiofiles.open('file.txt', mode='r') as f:
      content = await f.read()
  ```

---

### **2. 并发编程模型**
#### **① 多线程 + 线程池**
- **优化点**：
  - 避免频繁创建/销毁线程（使用线程池，如 Java `ThreadPoolExecutor`）。
  - 减少线程竞争（无锁数据结构、`ConcurrentHashMap`）。
- **缺点**：线程上下文切换开销大（C10K 问题）。

#### **② 协程（Coroutine）**
- **轻量级线程**：协程由用户态调度，切换成本极低。
- **实现**：
  - **Go**：Goroutine（原生支持，单机可轻松支持百万并发）。
  - **Python**：`asyncio` + `async/await`。
  - **Java**：Project Loom（虚拟线程，JDK 21+）。

#### **③ 事件驱动（Reactive）**
- **响应式编程**：如 Java 的 `Reactor`、`RxJava`，适合流式数据处理。

---

### **3. 连接与资源管理**
#### **① 连接复用**
- **数据库**：使用连接池（HikariCP、Druid）。
- **HTTP**：Keep-Alive 复用 TCP 连接。
- **gRPC**：多路复用 HTTP/2 连接。

#### **② 文件描述符优化**
- **调整系统限制**：
  
  ```bash
  # Linux 修改文件描述符限制
  ulimit -n 1000000  # 临时生效
  echo "* soft nofile 1000000" >> /etc/security/limits.conf  # 永久生效
  ```

---

### **4. 内存与数据结构优化**
#### **① 零拷贝技术**
- **减少数据拷贝**：如 Linux 的 `sendfile`、`splice`。
- **示例**：Nginx 静态文件发送直接使用 `sendfile`。

#### **② 高效数据结构**
- **缓存友好**：避免随机内存访问（如用数组替代链表）。
- **无锁结构**：如 `AtomicInteger`、`Disruptor` 环形队列。

---

### **5. 请求处理优化**
#### **① 异步化处理**
- **拆分耗时操作**：将阻塞操作（如数据库查询、文件 I/O）交给线程池或消息队列。
  ```java
  // Java 异步处理示例（CompletableFuture）
  CompletableFuture.supplyAsync(() -> queryDatabase(), executor)
                   .thenAccept(result -> sendResponse());
  ```

#### **② 批量处理**
- **合并请求**：如数据库批量插入（`INSERT INTO ... VALUES (...), (...)`）。
- **Redis Pipeline**：减少网络往返时间。

---

### **6. 缓存加速**
#### **① 内存缓存**
- **本地缓存**：Caffeine（Java）、`lru_cache`（Python）。
- **分布式缓存**：Redis（单机也可用）。

#### **② 静态资源缓存**
- **HTTP 缓存头**：`Cache-Control: max-age=3600`。

---

### **7. 限流与熔断**
#### **① 单机限流**
- **算法**：
  - **令牌桶**：Guava `RateLimiter`。
  - **漏桶**：`Semaphore`。
- **示例**：
  ```java
  // 每秒允许 1000 个请求
  RateLimiter limiter = RateLimiter.create(1000.0);
  if (limiter.tryAcquire()) {
      handleRequest();
  } else {
      return "Too many requests";
  }
  ```

#### **② 熔断降级**
- **规则**：错误率超阈值时拒绝请求（如 Netflix Hystrix）。

---

### **8. 语言与框架选择**
| 语言       | 高并发方案            | 适用场景     |
| ---------- | --------------------- | ------------ |
| **Go**     | Goroutine + Channel   | 通用高并发   |
| **Java**   | Netty/Vert.x + 线程池 | 网络服务     |
| **Python** | FastAPI + `asyncio`   | I/O 密集型   |
| **C++**    | `libevent` + 协程     | 高性能中间件 |

---

### **9. 性能调优工具**
- **Profiling**：
  - **Java**：Arthas、Async-Profiler。
  - **Python**：`cProfile`、`py-spy`。
- **系统监控**：
  - `top`、`vmstat`、`netstat`（Linux）。
  - **可视化**：Grafana + Prometheus。

---

### **总结：单机高并发关键点**
1. **I/O 模型**：非阻塞 + 多路复用（`epoll`/`kqueue`）。
2. **并发模型**：协程 > 线程池 > 多进程。
3. **资源管理**：连接复用、零拷贝、无锁设计。
4. **异步化**：耗时操作不阻塞主线程。
5. **缓存**：内存缓存加速热点数据。
6. **限流**：保护系统不被压垮。

**示例架构**（单机高性能 Web 服务器）：
```
Nginx（反向代理 + 静态资源）→ FastAPI（异步处理）→ Redis（缓存）→ MySQL（连接池）
```

通过合理组合上述技术，单机可轻松支持数万甚至百万级并发（如 Nginx/Redis 的基准测试）。



# Map 底层实现代码解析

Map（映射/字典）是编程中最常用的数据结构之一，不同语言有不同的实现方式。我将为您解析几种常见语言中Map的底层实现原理和关键代码逻辑。

## 1. Java HashMap 实现

Java的HashMap是基于哈希表实现的，JDK 8之后引入了链表转红黑树的优化。

### 核心数据结构

```java
// 数组+链表/红黑树结构
transient Node<K,V>[] table;

// 链表节点
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    V value;
    Node<K,V> next;
    // ...
}

// 红黑树节点
static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V> {
    TreeNode<K,V> parent;  // 父节点
    TreeNode<K,V> left;    // 左子树
    TreeNode<K,V> right;   // 右子树
    TreeNode<K,V> prev;    // 前驱节点
    boolean red;          // 颜色
    // ...
}
```

### 关键方法实现

#### put方法核心逻辑

```java
final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    // 1. 表为空则初始化
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    // 2. 计算索引位置，如果该位置为空则直接插入
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);
    else {
        Node<K,V> e; K k;
        // 3. 如果key已存在，则覆盖value
        if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
        // 4. 如果是树节点，则按树的方式插入
        else if (p instanceof TreeNode)
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        else {
            // 5. 链表遍历
            for (int binCount = 0; ; ++binCount) {
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null);
                    // 链表长度超过阈值(8)转为红黑树
                    if (binCount >= TREEIFY_THRESHOLD - 1)
                        treeifyBin(tab, hash);
                    break;
                }
                if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        // 6. 存在key则更新value
        if (e != null) {
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
    ++modCount;
    // 7. 超过容量则扩容
    if (++size > threshold)
        resize();
    afterNodeInsertion(evict);
    return null;
}
```

#### 扩容机制

```java
final Node<K,V>[] resize() {
    Node<K,V>[] oldTab = table;
    int oldCap = (oldTab == null) ? 0 : oldTab.length;
    int oldThr = threshold;
    int newCap, newThr = 0;
    // 计算新容量和阈值
    if (oldCap > 0) {
        if (oldCap >= MAXIMUM_CAPACITY) {
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                 oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1; // 双倍扩容
    }
    // 初始化
    else if (oldThr > 0)
        newCap = oldThr;
    else {
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
    // 创建新数组并重新哈希
    Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    table = newTab;
    // 数据迁移...
    return newTab;
}
```

## 2. C++ unordered_map 实现

C++的unordered_map也是基于哈希表实现，通常使用开链法解决冲突。

### 典型实现结构

```cpp
template<typename Key, typename Value,
         typename Hash = std::hash<Key>,
         typename KeyEqual = std::equal_to<Key>>
class unordered_map {
private:
    struct Node {
        Key key;
        Value value;
        Node* next;
        // ...
    };
    
    std::vector<Node*> buckets;  // 桶数组
    size_t element_count;        // 元素数量
    float max_load_factor;       // 最大负载因子
    
    // 哈希函数
    size_t hash(const Key& key) const {
        return Hash()(key) % buckets.size();
    }
    
public:
    // 接口方法...
};
```

### 关键操作实现

#### 插入操作伪代码

```cpp
std::pair<iterator, bool> insert(const value_type& value) {
    // 1. 计算哈希值
    size_t bucket_index = hash(value.first);
    
    // 2. 查找是否已存在
    Node* current = buckets[bucket_index];
    while (current != nullptr) {
        if (KeyEqual()(current->key, value.first)) {
            return {iterator(current), false}; // 已存在
        }
        current = current->next;
    }
    
    // 3. 创建新节点并插入链表头部
    Node* new_node = create_node(value);
    new_node->next = buckets[bucket_index];
    buckets[bucket_index] = new_node;
    ++element_count;
    
    // 4. 检查是否需要rehash
    if (load_factor() > max_load_factor) {
        rehash(buckets.size() * 2);
    }
    
    return {iterator(new_node), true};
}
```

## 3. Python dict 实现

Python的字典使用开放寻址法实现的哈希表，具有高度优化。

### CPython中的关键结构

```c
// Python字典条目
typedef struct {
    Py_hash_t me_hash;    // 缓存的哈希值
    PyObject *me_key;     // 键
    PyObject *me_value;   // 值
} PyDictKeyEntry;

// 字典结构
typedef struct _dictkeysobject {
    Py_ssize_t dk_refcnt;
    Py_ssize_t dk_size;    // 哈希表大小
    dict_lookup_func dk_lookup;
    Py_ssize_t dk_usable;  // 可用条目数
    PyDictKeyEntry dk_entries[1];  // 实际条目存储
} PyDictKeysObject;

typedef struct {
    PyObject_HEAD
    Py_ssize_t ma_used;     // 已用条目数
    PyDictKeysObject *ma_keys;  // 键集合
    PyObject **ma_values;   // 值数组(仅用于split table)
} PyDictObject;
```

### 关键操作逻辑

#### 查找过程

```c
static PyObject *
dict_get_item(PyObject *op, PyObject *key)
{
    Py_hash_t hash;
    PyDictObject *mp = (PyDictObject *)op;
    PyObject *value;
    
    // 计算哈希值
    hash = PyObject_Hash(key);
    if (hash == -1)
        return NULL;
        
    // 查找条目
    Py_ssize_t ix = (mp->ma_keys->dk_lookup)(mp, key, hash, &value);
    if (ix < 0)
        return NULL;
    return value;
}
```

#### 插入过程核心

```c
static int
insertdict(PyDictObject *mp, PyObject *key, Py_hash_t hash, PyObject *value)
{
    PyObject *old_value;
    PyDictKeyEntry *ep;
    
    // 查找现有条目
    Py_ssize_t ix = mp->ma_keys->dk_lookup(mp, key, hash, &old_value);
    
    if (ix == DKIX_ERROR)
        return -1;
        
    if (ix >= 0) {
        // 键已存在，更新值
        if (mp->ma_values) {
            assert(old_value != NULL);
            Py_INCREF(value);
            mp->ma_values[ix] = value;
            Py_DECREF(old_value);
        }
        else {
            ep = &mp->ma_keys->dk_entries[ix];
            Py_INCREF(value);
            Py_DECREF(ep->me_value);
            ep->me_value = value;
        }
        return 0;
    }
    
    // 需要插入新条目
    return insertdict_clean(mp, key, hash, value);
}
```

## 4. Go map 实现

Go语言的map使用哈希表实现，采用数组+链表结构。

### 运行时核心结构

```go
// Go map头部结构
type hmap struct {
    count     int    // 当前元素数量
    flags     uint8
    B         uint8  // 桶数量的对数(可容纳2^B个桶)
    noverflow uint16 // 溢出桶的大约数量
    hash0     uint32 // 哈希种子
    
    buckets    unsafe.Pointer // 2^B个桶的数组
    oldbuckets unsafe.Pointer // 扩容时的旧桶数据
    nevacuate  uintptr        // 扩容进度计数器
    
    extra *mapextra // 可选字段
}

// 桶结构
type bmap struct {
    tophash [bucketCnt]uint8 // 每个键哈希的高8位
    // 后面跟着bucketCnt个键和bucketCnt个值
    // 最后是一个溢出指针
}
```

### 关键操作实现

#### 查找过程核心

```go
func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer {
    // 计算哈希值
    hash := t.hasher(key, uintptr(h.hash0))
    m := bucketMask(h.B)
    b := (*bmap)(add(h.buckets, (hash&m)*uintptr(t.bucketsize)))
    
    // 检查旧桶是否需要迁移
    if c := h.oldbuckets; c != nil {
        if !h.sameSizeGrow() {
            m >>= 1
        }
        oldb := (*bmap)(add(c, (hash&m)*uintptr(t.bucketsize)))
        if !evacuated(oldb) {
            b = oldb
        }
    }
    
    // 遍历桶链表
    for ; b != nil; b = b.overflow(t) {
        for i := uintptr(0); i < bucketCnt; i++ {
            if b.tophash[i] != topHash(hash) {
                continue
            }
            k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize))
            if t.key.equal(key, k) {
                v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize))
                return v
            }
        }
    }
    return unsafe.Pointer(&zeroVal[0])
}
```

## 总结对比

| 特性         | Java HashMap                  | C++ unordered_map | Python dict      | Go map               |
| ------------ | ----------------------------- | ----------------- | ---------------- | -------------------- |
| **冲突解决** | 链表+红黑树                   | 链表              | 开放寻址         | 链表                 |
| **扩容策略** | 2倍扩容                       | 通常2倍           | 动态调整         | 2倍扩容              |
| **内存布局** | 数组+链表/树                  | 数组+链表         | 连续数组         | 数组+链表            |
| **线程安全** | 不安全(ConcurrentHashMap安全) | 不安全            | GIL保护(CPython) | 不安全(sync.Map安全) |
| **负载因子** | 默认0.75                      | 默认1.0           | 动态调整         | 6.5(触发扩容)        |

不同语言的map实现虽然细节不同，但核心思想都是基于哈希表，通过合理的哈希函数和冲突解决策略来提供高效的键值存取。



# python中哪些数据类型可以使用hash

在 Python 中，**可哈希（hashable）**的数据类型是指那些具有不可变性质、可以用作字典键或集合元素的数据类型。以下是 Python 中可哈希的数据类型及其特点：

---

### **1. 可哈希的数据类型**
#### **① 数值类型**
- `int`、`float`、`bool`、`complex`  
  
  ```python
  hash(42)        # 42
  hash(3.14)      # 322818021289917443
  hash(True)      # 1
  hash(1+2j)      # TypeError（复数不可哈希，但 CPython 中部分版本允许）
  ```
  > **注意**：复数的哈希行为取决于 Python 实现（CPython 3.10+ 会报错）。

#### **② 字符串（`str`）**
- 不可变的字符序列：
  ```python
  hash("hello")   # -1267296259
  ```

#### **③ 元组（`tuple`）**
- **仅当所有元素可哈希时**：
  ```python
  hash((1, "a"))  # 3713081631934410656
  hash((1, [2]))   # TypeError（因为包含不可哈希的列表）
  ```

#### **④ 冻结集合（`frozenset`）**
- 不可变的集合：
  ```python
  hash(frozenset({1, 2}))  # -1826646154
  ```

#### **⑤ `bytes`**
- 不可变的字节序列：
  ```python
  hash(b"abc")    # 1600820932997601
  ```

#### **⑥ `None`**
- 单例对象：
  ```python
  hash(None)      # 0
  ```

---

### **2. 不可哈希的数据类型**
以下类型是**可变**的，因此不可哈希：
- **`list`**：  
  ```python
  hash([1, 2])    # TypeError: unhashable type: 'list'
  ```
- **`dict`**：  
  
  ```python
  hash({"a": 1})  # TypeError: unhashable type: 'dict'
  ```
- **`set`**：  
  ```python
  hash({1, 2})    # TypeError: unhashable type: 'set'
  ```
- **`bytearray`**：  
  ```python
  hash(bytearray(b"abc"))  # TypeError
  ```

---

### **3. 自定义类的哈希行为**
默认情况下，用户定义的类是可哈希的（基于对象的内存地址）：
```python
class Person:
    pass

p = Person()
hash(p)  # 基于 id(p) 计算
```

#### **自定义哈希规则**
重写 `__hash__` 方法（需保证哈希值在对象生命周期内不变）：
```python
class Point:
    def __init__(self, x, y):
        self.x = x
        self.y = y
    
    def __hash__(self):
        return hash((self.x, self.y))  # 基于不可变属性计算

p = Point(1, 2)
hash(p)  # 3713081631934410656（与 hash((1, 2)) 相同）
```

> **重要规则**：  
> 如果重写了 `__hash__`，通常需要同时重写 `__eq__`，以确保两个相等的对象哈希值相同。

---

### **4. 哈希的用途**
1. **字典键**：  
   ```python
   d = {(1, 2): "value"}  # 元组作为键
   ```
2. **集合元素**：  
   ```python
   s = {1, "a", frozenset({2})}
   ```
3. **快速查找**：哈希表的时间复杂度为 O(1)。

---

### **5. 哈希冲突**
不同对象可能有相同的哈希值（冲突），Python 会自动处理：
```python
hash("hello") == hash(-729308063)  # 可能为 True，但字典/集合仍能区分
```

---

### **总结**
| **可哈希类型**         | **不可哈希类型** |
| ---------------------- | ---------------- |
| `int`, `float`, `bool` | `list`           |
| `str`                  | `dict`           |
| `tuple`（元素可哈希）  | `set`            |
| `frozenset`            | `bytearray`      |
| `bytes`                | 自定义可变对象   |
| `None`                 |                  |

**核心原则**：  
- **不可变对象通常可哈希**（如 `str`、`tuple`）。  
- **可变对象不可哈希**（如 `list`、`dict`）。  
- 自定义类默认可哈希，但重写 `__hash__` 时需保证逻辑一致性。



# python 中 hash过程

在 Python 中，**哈希（Hashing）**是将任意大小的数据映射为固定大小的唯一值（哈希值）的过程。哈希的核心目的是支持快速的数据查找和比较（如字典、集合的实现）。以下是 Python 中哈希过程的详细解析：

---

## **1. 哈希的基本原理**
### **哈希函数的作用**
- **输入**：任意对象（如整数、字符串、元组等）。
- **输出**：一个固定大小的整数（哈希值）。
- **关键特性**：
  - **确定性**：相同对象始终返回相同的哈希值。
  - **高效性**：计算速度快。
  - **均匀分布**：不同对象的哈希值应尽量均匀分布，减少冲突。

### **Python 中的哈希方法**
- 通过内置函数 `hash()` 调用对象的 `__hash__` 方法：
  ```python
  hash("hello")  # 返回一个整数，如 -1267296259
  ```

---

## **2. 哈希的实现过程**
### **① 内置类型的哈希逻辑**
#### **整数（`int`）**
- 直接返回整数值本身（除非是 `-1`，因与 Python 内部冲突处理机制冲突）。
  ```python
  hash(42)   # 42
  hash(-10)  # -10
  ```

#### **浮点数（`float`）**
- 基于 IEEE 754 二进制表示的哈希计算。
  ```python
  hash(3.14)  # 322818021289917443
  ```

#### **字符串（`str`）**
- 对每个字符的 Unicode 值进行多项式运算（避免相似字符串的哈希冲突）。
  ```python
  hash("hello")  # -1267296259
  hash("hellp")  # 完全不同（-1267296259 ≠ 新哈希值）
  ```

#### **元组（`tuple`）**
- 递归计算每个元素的哈希值，然后组合：
  ```python
  hash((1, "a"))  # 基于 hash(1) 和 hash("a") 的组合
  ```

#### **不可变集合（`frozenset`）**
- 对元素哈希值进行对称累加（顺序无关）。
  ```python
  hash(frozenset({1, 2})) == hash(frozenset({2, 1}))  # True
  ```

### **② 自定义类的哈希**
- 默认基于对象的内存地址（`id(obj)`）：
  ```python
  class Person:
      pass
  
  p = Person()
  hash(p)  # 等价于 id(p) // 16（CPython 实现细节）
  ```
- **自定义哈希**：重写 `__hash__` 方法（需与 `__eq__` 一致）：
  ```python
  class Point:
      def __init__(self, x, y):
          self.x = x
          self.y = y
      
      def __hash__(self):
          return hash((self.x, self.y))  # 基于属性计算
      
      def __eq__(self, other):
          return (self.x, self.y) == (other.x, other.y)
  
  p1 = Point(1, 2)
  p2 = Point(1, 2)
  hash(p1) == hash(p2)  # True
  ```

---

## **3. 哈希的用途**
### **① 字典（`dict`）的键**
- Python 字典使用哈希表实现，键的哈希值决定存储位置：
  ```python
  d = {"name": "Alice", "age": 30}  # 对 "name" 和 "age" 计算哈希
  ```

### **② 集合（`set`）的元素**
- 集合依赖哈希值去重：
  ```python
  s = {1, 2, 2, 3}  # 实际存储 {1, 2, 3}
  ```

### **③ 快速查找**
- 哈希表的时间复杂度为 **O(1)**（优于列表的 O(n)）。

---

## **4. 哈希冲突与解决**
### **冲突的原因**
- **不同对象可能哈希相同**（如哈希空间有限）：
  ```python
  hash("a")  # 12416037344
  hash(12416037344)  # 相同（但字典能区分，因比较实际值）
  ```

### **Python 的冲突处理**
1. **开放寻址法**（CPython 字典）：
   - 若哈希位置被占用，则探测下一个空闲位置。
2. **链表法**（Java `HashMap` 的风格，但 Python 不用）：
   - 哈希桶中存储链表（Python 仅在极端情况下使用）。

---

## **5. 哈希的限制与注意事项**
### **不可哈希的类型**
- **可变对象**（如 `list`、`dict`、`set`）不可哈希：
  ```python
  hash([1, 2])  # TypeError: unhashable type: 'list'
  ```

### **哈希的一致性**
- 若对象在生命周期内可变（如自定义类修改属性），则哈希值可能变化，导致字典/集合行为异常：
  ```python
  class Bad:
      def __init__(self, x):
          self.x = x
      
      def __hash__(self):
          return hash(self.x)
  
  b = Bad(1)
  d = {b: "value"}
  b.x = 2  # 修改后，字典可能无法找到键！
  ```

### **哈希攻击防护**
- Python 对字符串哈希引入**随机盐（salt）**，防止恶意构造大量冲突键导致性能退化（从 Python 3.3 开始）。

---

## **6. 哈希的底层实现（CPython）**
### **核心函数**
- `PyObject_Hash()`：调用对象的 `__hash__` 方法。
- 字符串哈希算法：PJW 哈希（Python 3.4+ 使用 SipHash 防止攻击）。

### **示例：字符串哈希计算**
```c
// CPython 源码（简化版）
static Py_hash_t
string_hash(PyObject *obj)
{
    Py_ssize_t len;
    const unsigned char *p;
    Py_hash_t x;
    
    p = (unsigned char *) PyUnicode_DATA(obj);
    len = PyUnicode_GET_LENGTH(obj);
    x = _Py_HashBytes(p, len);  // 实际计算
    return x;
}
```

---

## **总结**
| **关键点**       | **说明**                                             |
| ---------------- | ---------------------------------------------------- |
| **可哈希对象**   | `int`, `float`, `str`, `tuple`, `frozenset`, `bytes` |
| **不可哈希对象** | `list`, `dict`, `set`, `bytearray`                   |
| **自定义哈希**   | 重写 `__hash__` 和 `__eq__`，基于不可变属性          |
| **哈希冲突**     | Python 自动处理（开放寻址法）                        |
| **性能**         | O(1) 时间复杂度的查找（字典、集合）                  |
| **安全性**       | 字符串哈希使用随机盐防止攻击                         |

**使用建议**：  

- 字典键或集合元素必须可哈希（优先选择不可变类型）。  
- 自定义类若需哈希，确保属性不可变或哈希值稳定。  
- 避免在哈希计算中使用复杂逻辑（影响性能）。



# python中的深浅拷贝

在 Python 中，**深拷贝（deep copy）**和**浅拷贝（shallow copy）**是复制对象的两种方式，主要区别在于对**嵌套对象**的处理方式。以下是它们的核心区别和用法：

---

## **1. 浅拷贝（Shallow Copy）**
### **特点**
- **只复制最外层对象**，嵌套对象（如列表中的列表、字典中的字典）**仍引用原对象**。
- 修改嵌套对象时，原对象和拷贝对象会**同步变化**。

### **实现方式**
- **`copy.copy()`**  
- **切片操作**（如 `list[:]`）  
- **工厂函数**（如 `list()`, `dict()`）  

### **示例**
```python
import copy

original = [[1, 2], [3, 4]]
shallow = copy.copy(original)  # 或 original[:]

# 修改外层对象（独立）
shallow.append([5, 6])  
print(original)  # [[1, 2], [3, 4]]（未变）

# 修改嵌套对象（同步变化）
shallow[0][0] = 99
print(original)  # [[99, 2], [3, 4]]（被修改！）
```

---

## **2. 深拷贝（Deep Copy）**
### **特点**
- **递归复制所有嵌套对象**，完全独立于原对象。
- 修改任何层级的数据都不会影响原对象。

### **实现方式**
- **`copy.deepcopy()`**

### **示例**
```python
import copy

original = [[1, 2], [3, 4]]
deep = copy.deepcopy(original)

# 修改嵌套对象（原对象不变）
deep[0][0] = 99
print(original)  # [[1, 2], [3, 4]]（不受影响）
```

---

## **3. 关键对比**
| **场景**         | **浅拷贝**         | **深拷贝**                     |
| ---------------- | ------------------ | ------------------------------ |
| **外层对象修改** | 独立               | 独立                           |
| **嵌套对象修改** | 同步影响原对象     | 完全独立                       |
| **性能**         | 快（仅复制一层）   | 慢（递归复制所有层级）         |
| **适用场景**     | 简单对象（无嵌套） | 复杂对象（嵌套结构需完全独立） |

---

## **4. 常见误区**
### **① 直接赋值 vs 浅拷贝**
```python
a = [1, [2, 3]]
b = a          # 直接赋值（同一对象）
c = a[:]       # 浅拷贝（新对象，但嵌套部分共享）

a[0] = 99      # b同步变化，c不变
a[1][0] = 88   # b和c均同步变化
```

### **② 不可变类型的拷贝**
- 对不可变对象（如 `int`, `str`, `tuple`），浅拷贝和深拷贝无区别（因为无法修改）：
  ```python
  import copy
  a = (1, 2)
  b = copy.copy(a)     # 仍是原对象
  c = copy.deepcopy(a) # 仍是原对象
  print(b is a)        # True
  ```

---

## **5. 如何选择？**
- **用浅拷贝**：对象无嵌套或嵌套部分无需独立。
- **用深拷贝**：嵌套结构需要完全独立（如配置字典、复杂数据结构）。

---

## **6. 性能注意**
- 深拷贝可能递归复制大量数据，对复杂对象（如深度学习模型）慎用。
- 可通过自定义 `__deepcopy__` 方法优化深拷贝逻辑：
  ```python
  class MyClass:
      def __deepcopy__(self, memo):
          # 自定义深拷贝逻辑
          return MyClass(copy.deepcopy(self.data, memo))
  ```

---

**总结**：  

- **浅拷贝**是“一层复制”，**深拷贝**是“完全克隆”。  
- 根据数据结构的嵌套需求和性能要求选择合适方式。